{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c9d684",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "  微调Qwen-VL模型：本文采用finetune PEFT方法微调（参数高效微调）\n",
    "  方法：1、通过json标注图片正确答案的数据集，或者通过labelwise、labelImg、Label Studio标注平台进行标注\n",
    "       2、使用PEFT+LoRA方法微调模型\n",
    "  注： 1、没有使用QLoRA方法进行量化后微调模型\n",
    "      2、Qwen-VL微调框架地址：https://github.com/QwenLM/Qwen2.5-VL、https://github.com/QwenLM/Qwen2.5-VL/tree/main/qwen-vl-finetune\n",
    "\n",
    "主要有两种主流的微调方法，适用于不同的硬件条件和需求。\n",
    "1. 全参数微调（Full Fine-Tuning）\n",
    "    描述：更新模型的所有参数。\n",
    "    优点：通常能获得最好的性能，模型能充分学习新数据分布。\n",
    "    缺点：极其消耗显存，需要大量的GPU资源（可能需要多卡A100 80G及以上），训练成本高。\n",
    "    适用场景：数据量大、任务复杂、且拥有充足计算资源的情况。\n",
    "\n",
    "2. 参数高效微调（Parameter-Efficient Fine-Tuning, PEFT）\n",
    "这是目前最流行的微调方式，尤其是对于大模型。最常用的方法是 LoRA (Low-Rank Adaptation)。\n",
    "    描述：冻结原模型的所有权重，仅向模型中插入少量的可训练“旁路”矩阵（Adapter）来模拟全参数更新。\n",
    "    优点：\n",
    "        显存需求大幅降低：通常只需全参数微调 1/3 甚至更少的显存。\n",
    "        训练速度快：可训练参数少，计算量小。\n",
    "        产出小：最终只需要保存和分发小小的 Adapter 权重（几MB到几百MB），而不是整个模型（几个GB）。\n",
    "        可移植性强：一个基础模型可以搭配多个不同任务的Adapter。\n",
    "    缺点：性能可能略低于全参数微调（但在大多数情况下足够好）。\n",
    "    适用场景：绝大多数情况，特别是资源有限的个人开发者或实验室。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb8e991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 显示gradio和transformers的版本\n",
    "import gradio\n",
    "gradio.__version__\n",
    "import transformers \n",
    "transformers.__version__\n",
    "import torch\n",
    "torch.__version__\n",
    "import torch\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoTokenizer, AutoProcessor,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    default_data_collator\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9424d6a-9cc6-41f8-be8c-a13efa655c54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 调用Qwen-VL本地的模型进行微调\n",
    "model_name = \"/root/autodl-tmp/models/Qwen/Qwen-VL-Chat\"\n",
    "# ==================== 配置部分 ====================\n",
    "# LoRA 配置\n",
    "lora_config = LoraConfig(\n",
    "    r=64,  # LoRA的秩\n",
    "    lora_alpha=16,  # 缩放参数\n",
    "    target_modules=[  # 目标模块（针对Qwen-VL结构）\n",
    "        \"c_attn\",  # 注意力层的QKV投影\n",
    "        \"attn.c_proj\",  # 注意力输出投影\n",
    "        \"w1\",  # MLP层\n",
    "        \"w2\",\n",
    "        \"mlp.c_proj\",\n",
    "    ],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# 训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/root/autodl-tmp/models/Qwen/qwen-vl-lora-finetuned\",  # 输出目录\n",
    "    per_device_train_batch_size=1,  # 根据GPU显存调整（A100可尝试2-4）\n",
    "    gradient_accumulation_steps=8,  # 梯度累积步数\n",
    "    learning_rate=2e-4,  # 学习率\n",
    "    num_train_epochs=3,  # 训练轮数\n",
    "    logging_dir=\"./logs\",  # 日志目录\n",
    "    logging_steps=10,  # 日志记录步数\n",
    "    save_steps=100,  # 保存步数\n",
    "    fp16=True,  # 使用混合精度训练（如果GPU支持）\n",
    "    remove_unused_columns=False,  # 多模态训练必须为False，重要：不要自动移除未使用的列，否则会导致输入数据不一致\n",
    "    dataloader_pin_memory=False,  # 避免内存复制问题\n",
    ")\n",
    "\n",
    "# 性能优化参数，它的意思是将序列长度填充到指定的倍数，这样可以提高模型的效率\n",
    "def get_optimal_padding():\n",
    "    \"\"\"\n",
    "    根据硬件选择最优的pad_to_multiple_of值\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        # 检查GPU架构\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        if \"V100\" in gpu_name or \"A100\" in gpu_name:\n",
    "            return 64  # 新一代GPU\n",
    "        else:\n",
    "            return 8   # 一般GPU\n",
    "    else:\n",
    "        return 8       # CPU或其他设备\n",
    "\n",
    "# 数据路径\n",
    "dataset_path = \"qwen-vl-dataset.json\"  # 替换为你的数据集路径\n",
    "image_folder = \"./images\"  # 图片所在文件夹路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0e5f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== 数据预处理函数 ====================\n",
    "def format_conversation(conversations):\n",
    "    \"\"\"将对话历史格式化为Qwen-VL接受的文本格式\"\"\"\n",
    "    formatted_text = \"\"\n",
    "    for turn in conversations:\n",
    "        if turn[\"from\"] == \"user\":\n",
    "            formatted_text += f\"<|im_start|>user\\n{turn['value']}<|im_end|>\\n\"\n",
    "        elif turn[\"from\"] == \"assistant\":\n",
    "            formatted_text += f\"<|im_start|>assistant\\n{turn['value']}<|im_end|>\\n\"\n",
    "    # 添加assistant开始标记以引导生成\n",
    "    formatted_text += \"<|im_start|>assistant\\n\"\n",
    "    return formatted_text\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"处理批次数据的函数\"\"\"\n",
    "    images = []\n",
    "    texts = []\n",
    "    \n",
    "    for example in batch:\n",
    "        try:\n",
    "            # 1. 加载图片\n",
    "            if example.get('image'):\n",
    "                image_path = os.path.join(image_folder, example['image'])\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "                images.append(image)\n",
    "            else:\n",
    "                # 如果没有图片，使用空白图片或跳过\n",
    "                images.append(Image.new('RGB', (224, 224), (255, 255, 255)))\n",
    "            \n",
    "            # 2. 格式化文本\n",
    "            conversations = example['conversations']\n",
    "            formatted_text = format_conversation(conversations)\n",
    "            texts.append(formatted_text)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing example {example.get('id', 'unknown')}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # 3. 使用processor处理图像和文本\n",
    "    if images and texts:\n",
    "        inputs = processor(\n",
    "            images=images,\n",
    "            text=texts,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=2048,  # 根据需求调整\n",
    "            pad_to_multiple_of=get_optimal_padding(),  # 动态选择\n",
    "        )\n",
    "        \n",
    "        # 4. 设置labels（用于计算loss）\n",
    "        inputs[\"labels\"] = inputs[\"input_ids\"].clone()\n",
    "        \n",
    "        # 5. 将数据移动到GPU（如果可用）\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        return inputs\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598c6294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 加载模型和处理器\n",
    "print(\"Loading model and processor...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32, # 使用半精度节省显存\n",
    "    device_map=\"auto\",  # 多卡自动分配\n",
    "    trust_remote_code=True # Qwen系列需要\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    # 方法1: 使用eos_token作为pad_token\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    # 方法2 (备用): 添加新的pad_token\n",
    "    # tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    print(f\"Set pad_token to: {tokenizer.pad_token}\")\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    model_name, \n",
    "    trust_remote_code=True,\n",
    "    tokenizer=tokenizer  # 传入已配置好的tokenizer\n",
    ")\n",
    "\n",
    "# 检查配置是否正确\n",
    "print(f\"Final tokenizer pad_token: {tokenizer.pad_token}\")\n",
    "\n",
    "# 使用 tokenizer 处理纯文本\n",
    "# text = \"请描述这张图片的内容\"\n",
    "# encoded_text = tokenizer(text, return_tensors=\"pt\")\n",
    "# print(encoded_text.input_ids)  # 查看编码后的token IDs\n",
    "\n",
    "# 2. 应用LoRA\n",
    "print(\"Applying LoRA...\")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "    \n",
    "# 3. 加载数据集\n",
    "print(\"Loading dataset...\")\n",
    "dataset = load_dataset('json', data_files=dataset_path)['train']\n",
    "print(f\"成功加载数据集，包含 {len(dataset)} 条数据\")\n",
    "    \n",
    "# 4. 创建Trainer并开始训练\n",
    "print(\"Starting training...\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=collate_fn,\n",
    "    tokenizer=tokenizer,  # 直接使用配置好的tokenizer\n",
    ")\n",
    "    \n",
    "# 开始训练\n",
    "trainer.train()\n",
    "    \n",
    "# 5. 保存模型\n",
    "print(\"Saving model...\")\n",
    "# 调用Trainer对象的save_model方法，将经过微调后的模型保存到指定的输出目录。\n",
    "# 该方法会保存模型的权重参数以及配置文件，方便后续加载和使用。\n",
    "trainer.save_model()\n",
    "\n",
    "# 调用processor的save_pretrained方法，将处理器保存到训练参数中指定的输出目录。\n",
    "# 处理器包含了分词器、图像预处理等相关配置，保存后可在推理时使用相同的处理流程。\n",
    "processor.save_pretrained(training_args.output_dir)\n",
    "    \n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf861d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"微调后的推理示例\"\"\"\n",
    "from peft import PeftModel\n",
    "    \n",
    "# 加载基础模型\n",
    "base_model = AutoModelForVision2Seq.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "    \n",
    "# 加载LoRA权重\n",
    "model = PeftModel.from_pretrained(base_model, training_args.output_dir)\n",
    "    \n",
    "# 合并权重（可选，合并后推理更快）\n",
    "model = model.merge_and_unload()\n",
    "    \n",
    "#  加载处理器\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    training_args.output_dir, \n",
    "    trust_remote_code=True\n",
    ")\n",
    "    \n",
    "# 准备输入\n",
    "image_path = \"test_image.jpg\"  # 测试图片\n",
    "question = \"<image>\\n请描述这张图片。\"\n",
    "    \n",
    "image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "# 构建对话格式\n",
    "messages = [\n",
    "    {\"from\": \"user\", \"value\": question}\n",
    "]\n",
    "text = format_conversation(messages)\n",
    "    \n",
    "# 处理输入\n",
    "inputs = processor(\n",
    "    images=[image],\n",
    "    text=text,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "    \n",
    "# 生成回答（显式指定生成参数）\n",
    "# 调用模型的generate方法生成文本，传入处理后的输入数据以及生成参数\n",
    "# **inputs: 将处理后的输入数据以关键字参数的形式传入\n",
    "# max_new_tokens=512: 指定模型最多生成512个新的token\n",
    "# do_sample=True: 开启采样策略，使得生成结果更具多样性\n",
    "# temperature=0.7: 控制采样时的随机性，值越小生成结果越确定，值越大随机性越强\n",
    "# top_p=0.9: 使用核采样，只从概率累计和达到0.9的token中进行采样\n",
    "generated_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    ")\n",
    "    \n",
    "# 解码输出\n",
    "generated_text = processor.batch_decode(\n",
    "    generated_ids, \n",
    "    skip_special_tokens=True\n",
    ")[0]\n",
    "    \n",
    "# 打印结果\n",
    "print(\"Generated response:\", generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
